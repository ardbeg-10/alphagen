{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-08T10:53:39.051381Z",
     "start_time": "2024-08-08T10:53:36.411200Z"
    }
   },
   "source": [
    "from typing import TypeVar, Tuple\n",
    "from alphagen.data.expression import *\n",
    "\n",
    "from alphagen_qlib.stock_data import StockData\n",
    "from alphagen_generic.features import *\n",
    "\n",
    "import numpy as np\n",
    "from lightgbm import Booster\n",
    "from alphagen.utils.pytorch_utils import normalize_by_day\n",
    "from alphagen.data.calculator import AlphaCalculator\n",
    "from alphagen.utils.correlation import batch_pearsonr\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.utils import column_or_1d\n",
    "import lightgbm as lgb\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import optuna\n",
    "\n",
    "_T = TypeVar(\"_T\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T10:53:39.067381Z",
     "start_time": "2024-08-08T10:53:39.055382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_ensemble_alpha(exprs: List[Expression], model: Booster) -> Tensor:\n",
    "    n = len(exprs)\n",
    "    return torch.from_numpy(predict(model, exprs)).to(data.device)\n",
    "\n",
    "def predict(model: Booster, exprs: List[Expression]) -> np.ndarray:\n",
    "    X = torch.stack([_calc_alpha(expr) for expr in exprs], dim=-1).cpu().numpy()\n",
    "    X = X.reshape(-1, X.shape[-1])\n",
    "    val = model.predict(X)\n",
    "    return unstack(val)\n",
    "\n",
    "def unstack(value: np.ndarray) -> np.ndarray:\n",
    "    return value.reshape(data.n_days, data.n_stocks)\n",
    "\n",
    "def _calc_alpha(expr: Expression) -> Tensor:\n",
    "    return normalize_by_day(expr.evaluate(data))\n",
    "\n",
    "def _calc_ICs(value1: Tensor, value2: Tensor) -> Tensor:\n",
    "    return batch_pearsonr(value1, value2)\n",
    "\n",
    "def _calc_IC(value1: Tensor, value2: Tensor) -> float:\n",
    "    return batch_pearsonr(value1, value2).mean().item()\n",
    "\n",
    "def _calc_IR(value1: Tensor, value2: Tensor) -> float:\n",
    "    ICs = _calc_ICs(value1, value2)\n",
    "    IC_mean = ICs.mean().item()\n",
    "    IC_std = ICs.std().item()\n",
    "    epsilon = 1e-10  # 防止除以零的小值\n",
    "    IR = IC_mean / (IC_std - epsilon)\n",
    "    return IR\n",
    "\n",
    "def test_ensemble(exprs: List[Expression], model: Booster, calculator: AlphaCalculator) -> Tuple[float, float]:\n",
    "    return calc_pool_all_ret(exprs, calculator.target_value, model)\n",
    "\n",
    "def calc_pool_all_ret(exprs: List[Expression], target: Tensor, model: Booster) -> Tuple[float, float]:\n",
    "    with torch.no_grad():\n",
    "        ensemble_value = make_ensemble_alpha(exprs, model)\n",
    "        return _calc_IC(ensemble_value, target), _calc_IR(ensemble_value, target)"
   ],
   "id": "dff142a1a34d95ad",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-08-08T10:53:39.068381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from alphagen_qlib.utils import load_alpha_pool_by_path, load_dt_model_by_path\n",
    "from alphagen_qlib.calculator import QLibStockDataCalculator\n",
    "\n",
    "POOL_PATH = 'model/51200_steps_pool.json'\n",
    "DT_PATH = 'model/51200_steps_dt.txt'\n",
    "\n",
    "data = StockData(instrument='csi500',\n",
    "                 start_time='2024-06-01',\n",
    "                 end_time='2024-07-01',\n",
    "                 max_future_days=1,\n",
    "                 )\n",
    "\n",
    "close = Feature(FeatureType.CLOSE)\n",
    "target = Ref(close, -1) / close - 1"
   ],
   "id": "d51f2340d35a684a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[46160:MainThread](2024-08-08 18:53:39,316) INFO - qlib.Initialization - [config.py:416] - default_conf: client.\n",
      "[46160:MainThread](2024-08-08 18:53:39,868) INFO - qlib.Initialization - [__init__.py:74] - qlib successfully initialized based on client settings.\n",
      "[46160:MainThread](2024-08-08 18:53:39,869) INFO - qlib.Initialization - [__init__.py:76] - data_path={'__DEFAULT_FREQ': WindowsPath('C:/Users/liush/.qlib/qlib_data/cn_data_rolling')}\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "calculator = QLibStockDataCalculator(data=data, target=target)\n",
    "exprs, _ = load_alpha_pool_by_path(POOL_PATH)\n",
    "booster = load_dt_model_by_path(DT_PATH)\n",
    "\n",
    "ensemble_alpha = make_ensemble_alpha(exprs, booster)\n",
    "df = data.make_dataframe(ensemble_alpha)\n",
    "\n",
    "print(test_ensemble(exprs, booster, calculator))"
   ],
   "id": "107e39e7ead71e3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# def get_data(exprs: Expression, target_value: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "#     X = torch.stack([_calc_alpha(expr) for expr in exprs], dim=-1).cpu().numpy()\n",
    "#     X = X.reshape(-1, X.shape[-1])\n",
    "#     y = column_or_1d(target_value.cpu().numpy().reshape(-1, 1))\n",
    "#     return X, y\n",
    "# \n",
    "# def objective(trial):\n",
    "#     data, target = get_data(exprs, calculator.target_value)\n",
    "#     train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n",
    "#     dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "# \n",
    "#     param = {\n",
    "#         \"objective\": \"regression\",\n",
    "#         \"metric\": \"huber\",\n",
    "#         \"verbosity\": -1,\n",
    "#         \"boosting_type\": \"dart\",\n",
    "#         #\"drop_rate\":0.45,\n",
    "#         #\"max_drop\":30,\n",
    "#         \"skip_drop\":0.65,\n",
    "#         #\"max_bin\":65,\n",
    "#         #\"bagging_fraction\": 0.9,\n",
    "#         #\"bagging_freq\": 4,\n",
    "#         #\"feature_fraction\": 0.45,\n",
    "#         #\"min_split_gain\": 0.67,\n",
    "#         \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 100, 5000),\n",
    "#     }\n",
    "# \n",
    "#     gbm = lgb.train(param, dtrain)\n",
    "#     preds = gbm.predict(valid_x)\n",
    "#     pred_labels = np.rint(preds)\n",
    "#     accuracy = sklearn.metrics.mean_absolute_error(valid_y, pred_labels)\n",
    "#     return accuracy\n",
    "# \n",
    "# def study():\n",
    "#     study = optuna.create_study(direction=\"minimize\")\n",
    "#     study.optimize(objective, n_trials=25)\n",
    "# \n",
    "#     print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "# \n",
    "#     print(\"Best trial:\")\n",
    "#     trial = study.best_trial\n",
    "# \n",
    "#     print(\"  Value: {}\".format(trial.value))\n",
    "# \n",
    "#     print(\"  Params: \")\n",
    "#     for key, value in trial.params.items():\n",
    "#         print(\"    {}: {}\".format(key, value))\n",
    "#         \n",
    "# study()"
   ],
   "id": "f9ffb4d9a180a43f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def train_lgbm(exprs: List[Expression], pretrained: Booster, target_value: Tensor) -> Booster:\n",
    "    n_splits = 5\n",
    "    X = torch.stack([_calc_alpha(expr) for expr in exprs], dim=-1).cpu().numpy()\n",
    "    X = X.reshape(-1, X.shape[-1])\n",
    "    y = column_or_1d(target_value.cpu().numpy().reshape(-1, 1))\n",
    "\n",
    "    threshold = 3\n",
    "\n",
    "    X = np.where(X > threshold, threshold, X)\n",
    "    X = np.where(X < -threshold, -threshold, X)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    best_model = None\n",
    "    best_score = float('inf')\n",
    "\n",
    "    for train_index, valid_index in kf.split(X):\n",
    "        X_train, X_valid = X[train_index], X[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        \n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n",
    "\n",
    "\n",
    "        params = {\n",
    "            'objective': 'regression',  # 根据你的实际任务调整\n",
    "            'num_leaves': 31,\n",
    "            'max_depth': 6,\n",
    "            #'learning_rate': 0.12,\n",
    "            'metric': 'l1, l2',\n",
    "            \"boosting\": 'dart',\n",
    "            \"lambda_l1\": 9,\n",
    "            \"lambda_l2\": 2,\n",
    "            \"skip_drop\":0.65,\n",
    "            #\"max_bin\":65,\n",
    "            \"bagging_fraction\": 0.9,\n",
    "            \"bagging_freq\": 5,\n",
    "            \"feature_fraction\": 0.8879,\n",
    "            #\"min_gain_to_split\": 0.67,\n",
    "            #\"extra_trees\": True,\n",
    "            #\"min_data_in_leaf\": 1000,\n",
    "            \"verbosity\": 1,\n",
    "        }\n",
    "        # 继续训练模型\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            num_boost_round=100,  \n",
    "            valid_sets=[valid_data],\n",
    "            init_model=None \n",
    "        )\n",
    "\n",
    "        score = model.best_score['valid_0']['l1']\n",
    "        # 计算训练误差\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "\n",
    "        # 计算测试误差\n",
    "        y_test_pred = model.predict(X_valid)\n",
    "        test_rmse = mean_squared_error(y_valid, y_test_pred, squared=False)\n",
    "\n",
    "        print('\\n')\n",
    "        print('\\n')\n",
    "        print(f'Train RMSE: {train_rmse}')\n",
    "        print(f'Test RMSE: {test_rmse}')\n",
    "        print('\\n')\n",
    "        print('\\n')\n",
    "\n",
    "        best_model = model\n",
    "\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "\n",
    "    return best_model\n"
   ],
   "id": "320a405ffe70c550",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "booster = train_lgbm(exprs, booster, calculator.target_value)\n",
    "print(test_ensemble(exprs, booster, calculator))"
   ],
   "id": "e54ecd64bd87c540",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "booster.save_model('model/boostered_7.txt')",
   "id": "dd64dd2c32949c37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "booster.feature_importance()",
   "id": "2ef52860421e528",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "241862aeb60a23de",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
