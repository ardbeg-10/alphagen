{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-12T06:48:05.997740Z",
     "start_time": "2024-08-12T06:48:03.795635Z"
    }
   },
   "source": [
    "from typing import TypeVar, Tuple\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from alphagen.data.expression import *\n",
    "\n",
    "from alphagen_qlib.stock_data import StockData\n",
    "from alphagen_generic.features import *\n",
    "\n",
    "import numpy as np\n",
    "from lightgbm import Booster\n",
    "from alphagen.utils.pytorch_utils import normalize_by_day\n",
    "from alphagen.data.calculator import AlphaCalculator\n",
    "from alphagen.utils.correlation import batch_pearsonr\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.utils import column_or_1d\n",
    "import lightgbm as lgb\n",
    "\n",
    "_T = TypeVar(\"_T\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T06:48:06.013738Z",
     "start_time": "2024-08-12T06:48:05.999739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_ensemble_alpha(exprs: List[Expression], model: Booster) -> Tensor:\n",
    "    n = len(exprs)\n",
    "    return torch.from_numpy(predict(model, exprs)).to(data.device)\n",
    "\n",
    "def predict(model: Booster, exprs: List[Expression]) -> np.ndarray:\n",
    "    X = torch.stack([_calc_alpha(expr) for expr in exprs], dim=-1).cpu().numpy()\n",
    "    X = X.reshape(-1, X.shape[-1])\n",
    "    val = model.predict(X)\n",
    "    return unstack(val)\n",
    "\n",
    "def unstack(value: np.ndarray) -> np.ndarray:\n",
    "    return value.reshape(data.n_days, data.n_stocks)\n",
    "\n",
    "def _calc_alpha(expr: Expression) -> Tensor:\n",
    "    return normalize_by_day(expr.evaluate(data))\n",
    "\n",
    "def _calc_ICs(value1: Tensor, value2: Tensor) -> Tensor:\n",
    "    return batch_pearsonr(value1, value2)\n",
    "\n",
    "def _calc_IC(value1: Tensor, value2: Tensor) -> float:\n",
    "    return batch_pearsonr(value1, value2).mean().item()\n",
    "\n",
    "def _calc_IR(value1: Tensor, value2: Tensor) -> float:\n",
    "    ICs = _calc_ICs(value1, value2)\n",
    "    IC_mean = ICs.mean().item()\n",
    "    IC_std = ICs.std().item()\n",
    "    epsilon = 1e-10  # 防止除以零的小值\n",
    "    IR = IC_mean / (IC_std - epsilon)\n",
    "    return IR\n",
    "\n",
    "def test_ensemble(exprs: List[Expression], model: Booster, calculator: AlphaCalculator) -> Tuple[float, float]:\n",
    "    return calc_pool_all_ret(exprs, calculator.target_value, model)\n",
    "\n",
    "def calc_pool_all_ret(exprs: List[Expression], target: Tensor, model: Booster) -> Tuple[float, float]:\n",
    "    with torch.no_grad():\n",
    "        ensemble_value = make_ensemble_alpha(exprs, model)\n",
    "        return _calc_IC(ensemble_value, target), _calc_IR(ensemble_value, target)"
   ],
   "id": "dff142a1a34d95ad",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T07:14:57.667352Z",
     "start_time": "2024-08-12T07:14:42.047274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from alphagen_qlib.utils import load_alpha_pool_by_path, load_dt_model_by_path\n",
    "from alphagen_qlib.calculator import QLibStockDataCalculator\n",
    "\n",
    "POOL_PATH = 'model/10240_steps_pool.json'\n",
    "DT_PATH = 'model/10240_steps_dt.txt'\n",
    "\n",
    "data = StockData(instrument='csi500',\n",
    "                 start_time='2023-01-01',\n",
    "                 end_time='2024-01-01',\n",
    "                 max_future_days=21,\n",
    "                 )\n",
    "\n",
    "close = Feature(FeatureType.CLOSE)\n",
    "target = Ref(close, -1) / close - 1"
   ],
   "id": "d51f2340d35a684a",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T07:14:58.141943Z",
     "start_time": "2024-08-12T07:14:57.669352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "calculator = QLibStockDataCalculator(data=data, target=target)\n",
    "exprs, _ = load_alpha_pool_by_path(POOL_PATH)\n",
    "booster = load_dt_model_by_path(DT_PATH)\n",
    "\n",
    "ensemble_alpha = make_ensemble_alpha(exprs, booster)\n",
    "df = data.make_dataframe(ensemble_alpha)\n",
    "\n",
    "print(test_ensemble(exprs, booster, calculator))"
   ],
   "id": "107e39e7ead71e3f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.08638456565692526, 0.6852765260446161)\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T07:15:02.149177Z",
     "start_time": "2024-08-12T07:15:02.142665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import optuna\n",
    "# import sklearn\n",
    "# \n",
    "# def get_data(exprs: Expression, target_value: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "#     X = torch.stack([_calc_alpha(expr) for expr in exprs], dim=-1).cpu().numpy()\n",
    "#     X = X.reshape(-1, X.shape[-1])\n",
    "#     y = column_or_1d(target_value.cpu().numpy().reshape(-1, 1))\n",
    "#     return X, y\n",
    "# \n",
    "# def objective(trial):\n",
    "#     data, target = get_data(exprs, calculator.target_value)\n",
    "#     train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n",
    "#     dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "# \n",
    "#     param = {\n",
    "#         \"objective\": \"regression\",\n",
    "#         \"metric\": \"rmse\",\n",
    "#         \"verbosity\": -1,\n",
    "#         \"boosting_type\": \"dart\",\n",
    "#         \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 1.0),\n",
    "#         \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 1.0),\n",
    "#         \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "#         \"max_bin\": trial.suggest_int(\"max_bin\", 32, 512),\n",
    "#         \"max_depth\": trial.suggest_int(\"max_depth\", 4, 10),\n",
    "#         \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "#         \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "#         \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "#         \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 3000),\n",
    "#     }\n",
    "# \n",
    "#     gbm = lgb.train(param, dtrain)\n",
    "#     preds = gbm.predict(valid_x)\n",
    "#     pred_labels = np.rint(preds)\n",
    "#     accuracy = sklearn.metrics.mean_squared_error(valid_y, pred_labels)\n",
    "#     return accuracy\n",
    "# \n",
    "# def study():\n",
    "#     study = optuna.create_study(direction=\"minimize\")\n",
    "#     study.optimize(objective, n_trials=20)\n",
    "# \n",
    "#     print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "# \n",
    "#     print(\"Best trial:\")\n",
    "#     trial = study.best_trial\n",
    "# \n",
    "#     print(\"  Value: {}\".format(trial.value))\n",
    "# \n",
    "#     print(\"  Params: \")\n",
    "#     for key, value in trial.params.items():\n",
    "#         print(\"    {}: {}\".format(key, value))\n",
    "# \n",
    "# study()"
   ],
   "id": "f9ffb4d9a180a43f",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T07:15:02.606541Z",
     "start_time": "2024-08-12T07:15:02.583530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "def train_lgbm(exprs: List[Expression], pretrained: Booster, target_value: Tensor) -> Booster:\n",
    "    n_splits = 2\n",
    "    X = torch.stack([_calc_alpha(expr) for expr in exprs], dim=-1).cpu().numpy()\n",
    "    X = X.reshape(-1, X.shape[-1])\n",
    "    y = column_or_1d(target_value.cpu().numpy().reshape(-1, 1))\n",
    "\n",
    "    threshold = 3\n",
    "\n",
    "    X = np.where(X > threshold, threshold, X)\n",
    "    X = np.where(X < -threshold, -threshold, X)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    best_model = None\n",
    "    best_score = float('inf')\n",
    "\n",
    "    for train_index, valid_index in kf.split(X):\n",
    "        X_train, X_valid = X[train_index], X[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        \n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n",
    "\n",
    "\n",
    "        params = {\n",
    "            'objective': 'regression',  # 根据你的实际任务调整\n",
    "            'num_leaves': 31,\n",
    "            'max_depth': 6,\n",
    "            'metric': 'rmse',\n",
    "            \"boosting\": 'dart',\n",
    "            \"lambda_l1\": 9,\n",
    "            \"lambda_l2\": 0.05,\n",
    "            \"skip_drop\":0.65,\n",
    "            \"max_bin\":65,\n",
    "            \"bagging_fraction\": 0.9,\n",
    "            \"bagging_freq\": 5,\n",
    "            \"feature_fraction\": 0.8879,\n",
    "            \"min_data_in_leaf\": 20,\n",
    "        }   \n",
    "        # 继续训练模型\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            num_boost_round=100,  \n",
    "            valid_sets=[valid_data],\n",
    "            init_model=pretrained, \n",
    "        )\n",
    "\n",
    "        score = model.best_score['valid_0']['rmse']\n",
    "        # 计算训练误差\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        train_rmse = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "        # 计算测试误差\n",
    "        y_test_pred = model.predict(X_valid)\n",
    "        test_rmse = mean_absolute_error(y_valid, y_test_pred)\n",
    "\n",
    "        print('\\n')\n",
    "        print('\\n')\n",
    "        print(f'Train RMSE: {train_rmse}')\n",
    "        print(f'Test RMSE: {test_rmse}')\n",
    "        print('\\n')\n",
    "        print('\\n')\n",
    "\n",
    "        best_model = model\n",
    "\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "\n",
    "    return best_model\n"
   ],
   "id": "320a405ffe70c550",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-08-12T07:15:05.588012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "booster = train_lgbm(exprs, booster, calculator.target_value)\n",
    "print(test_ensemble(exprs, booster, calculator))"
   ],
   "id": "e54ecd64bd87c540",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4095\n",
      "[LightGBM] [Info] Number of data points in the train set: 60500, number of used features: 63\n",
      "[LightGBM] [Warning] Detected that num_threads changed during training (from 14 to 12), it may cause unexpected errors.\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T07:07:15.498965Z",
     "start_time": "2024-08-12T07:07:15.470370Z"
    }
   },
   "cell_type": "code",
   "source": "booster.save_model('model/boostered_3.txt')",
   "id": "dd64dd2c32949c37",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x1d3adecf220>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T07:08:53.578342Z",
     "start_time": "2024-08-12T07:08:53.565832Z"
    }
   },
   "cell_type": "code",
   "source": "booster.feature_importance()",
   "id": "2ef52860421e528",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 18,  18, 133,   4, 163, 316, 393, 102,   9, 269, 107,  76,  18,\n",
       "        40,   2,  18, 143,  77, 119,  60, 134, 128, 192, 201,  34,   0,\n",
       "         0,  21,  52,  15, 209,  52,  79,   5, 145,   0, 240,   8, 143,\n",
       "         3, 292,  73,   1,   0,  89,   4, 112, 167,  31, 121,  80, 248,\n",
       "        28,  30,  43,  57,   0,   1,   0,  90,  74, 170, 209, 293,  29])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d9381fcab7c21bb6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
